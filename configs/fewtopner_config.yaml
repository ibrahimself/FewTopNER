# Model Configuration
model:
  model_name: "xlm-roberta-base"
  hidden_size: 768
  dropout: 0.1
  max_length: 512
  projection_size: 512
  entity_feature_size: 256
  topic_feature_size: 256
  prototype_dim: 128
  contrastive_dim: 128
  num_entity_labels: 9
  num_topics: 100
  num_attention_heads: 8
  intermediate_size: 3072
  hidden_act: "gelu"
  attention_dropout: 0.1
  languages: ["en", "fr", "de", "es", "it"]
  language_adapters: true
  use_language_embeddings: true
  language_embedding_dim: 32
  cross_lingual_sharing: true
  use_crf: true
  use_language_adapters: true
  shared_encoder_layers: 12
  task_specific_layers: 2

# Preprocessing Configuration
preprocessing:
  wikineural_path: "data/ner/wikineural"
  wikipedia_path: "data/topic/wikipedia"
  wiki_dump_date: "20231101"
  max_ner_length: 128
  min_text_length: 100
  max_text_length: 1000
  min_word_freq: 5
  max_word_freq: 0.7
  max_vocab_size: 50000
  cache_dir: "cache"
  use_cache: true

# Data Configuration
data:
  train_ratio: 0.8
  val_ratio: 0.1
  train_batch_size: 32
  eval_batch_size: 64
  support_batch_size: 16
  query_batch_size: 16
  n_way: 5
  k_shot: 5
  n_query: 10
  min_examples_per_class: 20
  num_languages_per_episode: 3
  num_workers: 4
  pin_memory: true

# Optimizer Configuration
optimizer:
  encoder_lr: 2e-5
  task_lr: 1e-4
  bridge_lr: 1e-4
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  weight_decay: 0.01
  adam_epsilon: 1e-8
  beta1: 0.9
  beta2: 0.999

# Training Configuration
training:
  num_epochs: 10
  episodes_per_epoch: 100
  val_episodes: 50
  test_episodes: 100
  entity_loss_weight: 1.0
  topic_loss_weight: 1.0
  bridge_loss_weight: 0.1
  contrastive_loss_weight: 0.1
  prototype_loss_weight: 0.1
  device: "cuda"
  fp16: false
  output_dir: "outputs"
  save_steps: 1000
  save_total_limit: 5
  logging_steps: 100
  evaluation_strategy: "epoch"
  log_level: "info"
  use_wandb: true

# Bridge Configuration
bridge:
  bridge_num_heads: 8
  attention_dropout: 0.1
  temperature: 0.07
  use_language_adapters: true
  use_task_gates: true
  gate_dropout: 0.1
  alignment_weight: 0.1
  consistency_weight: 0.1

# Evaluation Configuration
evaluation:
  use_seqeval: true
  entity_scheme: "BIO2"
  compute_coherence: true
  compute_diversity: true
  evaluate_transfer: true
  source_languages: ['en']
  eval_episodes: 1000